{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "\n",
    "import sentence_utils as ut\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "# from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../../../../tasks/07-language-as-sequence/run-on-test.json\"\n",
    "test_X = []\n",
    "test_y = []\n",
    "with open(test_path) as input_f:\n",
    "    results = json.loads(input_f.read())\n",
    "    for result in results:\n",
    "        X = [s[0] for s in result]\n",
    "        y = [s[1] for s in result]\n",
    "        test_X.append(X)\n",
    "        test_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = zipfile.ZipFile('./data.zip')\n",
    "\n",
    "def read_lines(file):\n",
    "    with input_file.open(file) as inp:\n",
    "        return inp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_trailing_fullstops(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    if re.search(\"[?!\\.]\\s?$\", sentence, re.MULTILINE):\n",
    "        return sentence\n",
    "    return re.sub(\"\\.?\\s?$\", \".\", sentence, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrn_sentences = read_lines('enron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrn_sentences_normalized = [normalize_trailing_fullstops(sent) for sent in enrn_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentences = read_lines('newsgroup.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentences_normalized = [normalize_trailing_fullstops(sent) for sent in news_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = read_lines('wikipedia.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences_normalized = [normalize_trailing_fullstops(sent) for sent in wiki_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_finishing_punctutation(char):\n",
    "    return char in ['.', '..', '...', '?', '!']\n",
    "def remove_trailing_punctutation(pair):\n",
    "    tok_sequence, label_sequence = pair\n",
    "    if is_finishing_punctutation(tok_sequence[-1]):\n",
    "        remove_count = 0\n",
    "        for i in range(-1, -len(tok_sequence), -1):\n",
    "            if is_finishing_punctutation(tok_sequence[i]):\n",
    "                remove_count += 1\n",
    "            else:\n",
    "                break\n",
    "        for i in range(remove_count):\n",
    "            tok_sequence.pop()\n",
    "            label_sequence.pop()\n",
    "    return (tok_sequence, label_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to build simple baseline, but first need to add some positive cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489370"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_positive_cases(Xs, ys, ratio=0.4):\n",
    "    \"\"\"Merging randomly sampled sentences and adding positive cases\"\"\"\n",
    "    newXs = []\n",
    "    newYs = []\n",
    "    for i in range(math.floor(len(Xs) * ratio)):\n",
    "        first_id = random.randint(0, len(Xs) - 1)\n",
    "        second_id = random.randint(0, len(Xs) - 1)\n",
    "        while first_id == second_id:\n",
    "            second_id = random.randint(0, len(Xs) - 1)\n",
    "        first_X, first_y = Xs[first_id].copy(), ys[first_id].copy()\n",
    "        second_X, second_y = Xs[second_id].copy(), ys[second_id].copy()\n",
    "        first_X, first_y = remove_trailing_punctutation((first_X, first_y))\n",
    "        first_y[-1] = True\n",
    "        newXs.append(first_X + second_X)\n",
    "        newYs.append(first_y + second_y)\n",
    "    return Xs + newXs, ys + newYs\n",
    "\n",
    "def augment_partially(Xs, ys):\n",
    "    \"\"\"Idea here is to do some augmentations on Xs like changing names / exclamations / farewells\"\"\"\n",
    "    newXs = []\n",
    "    newYs = []\n",
    "    return Xs + newXs, ys + newYs\n",
    "\n",
    "def create_dataset(dataset_sentences, positive_cases_ration):\n",
    "    raw_pairs = [ut.sentence_to_sequences(s) for s in dataset_sentences]\n",
    "    [Xs, ys] = zip(*raw_pairs)\n",
    "    Xs = list(Xs)\n",
    "    ys = list(ys)\n",
    "    Xs, ys = add_positive_cases(Xs, ys, positive_cases_ration)\n",
    "    Xs, ys = augment_partially(Xs, ys)\n",
    "    return Xs, ys\n",
    "\n",
    "Xs, ys = create_dataset(enrn_sentences_normalized, 0.4)\n",
    "len(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Here', 'is', 'our', 'forecast', '.'],\n",
       " ['test', 'successful', '.'],\n",
       " ['way', 'to', 'go', '!', '!']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BaselinePredictor:\n",
    "    def _predict_single(self, x):\n",
    "        res = [False]\n",
    "        for (prev_tok, tok) in zip(x, x[1:]):\n",
    "            if prev_tok and tok[0].isupper() and not is_finishing_punctutation(prev_tok):\n",
    "                res.append(True)\n",
    "            else:\n",
    "                res.append(False)\n",
    "        return res\n",
    "    \n",
    "    def predict(self, Xs):\n",
    "        return [self._predict_single(x) for x in Xs]\n",
    "        \n",
    "baseline = BaselinePredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline.predict(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.82      0.90   8050555\n",
      "       True       0.02      0.19      0.03    139820\n",
      "\n",
      "avg / total       0.97      0.81      0.88   8190375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(flatten(ys), flatten(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " precision    recall  f1-score   support\n",
    "\n",
    "      False       0.98      0.82      0.90   8050279\n",
    "       True       0.02      0.19      0.03    139820\n",
    "\n",
    "avg / total       0.97      0.81      0.88   8190099\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# well, given we are thinking about using different datasets, might as well try running baseline on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.97      0.93      0.95      4542\n",
      "       True       0.03      0.06      0.04       155\n",
      "\n",
      "avg / total       0.94      0.91      0.92      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = baseline.predict(test_X)\n",
    "print(classification_report(flatten(test_y), flatten(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.97      0.93      0.95      4542\n",
    "       True       0.03      0.06      0.04       155\n",
    "\n",
    "avg / total       0.94      0.91      0.92      4697\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it has slightly more precision but multiple times lower recall for minor class. \n",
    "Let's now actually build some predictor, a simple one using subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_quote(word):\n",
    "    return re.match(\"[\\\"\\'¬´¬ª‚Äù‚Äú]\", word) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "doc = ut.toks_to_spacy(nlp, ut.sentence_to_tokens(\"Bill is really nice grello\"))\n",
    "for (i, tok) in enumerate(doc):\n",
    "    print(tok.like_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_tok_to_feature(label, tok):\n",
    "    if not tok:\n",
    "        return {\n",
    "            label + '_shape': 'NONE',\n",
    "            label + '_is_quote': False,\n",
    "            label + '_is_punct': False,\n",
    "            label + '_is_start': False,\n",
    "            label + '_in_vocab': False,\n",
    "            label + '_like_num': False,\n",
    "            label + '_like_url': False,\n",
    "            label + '_like_email': False,\n",
    "            label + '_ent_type': 'NONE',\n",
    "        }\n",
    "    return {\n",
    "        label + '_shape': tok.shape_[0:4],\n",
    "        label + '_is_quote': is_quote(str(tok)),\n",
    "        label + '_is_punct': tok.is_punct,\n",
    "        label + '_is_start': tok.is_sent_start,\n",
    "        label + '_in_vocab': str(tok) in nlp.vocab,\n",
    "        label + '_like_num': tok.like_num,\n",
    "        label + '_like_url': tok.like_url,\n",
    "        label + '_like_email': tok.like_email,\n",
    "        label + '_ent_type': tok.ent_type_\n",
    "        \n",
    "    }\n",
    "def to_features(word, prev_1, next_1): # prev_2, prev_3, next_2, next_3\n",
    "    return {\n",
    "        **single_tok_to_feature('word', word),\n",
    "        **single_tok_to_feature('prev_1', prev_1),\n",
    "        **single_tok_to_feature('next_1', next_1),\n",
    "    }\n",
    "def single_doc_to_features(sentence):\n",
    "    sent = [tok for tok in sentence if tok]\n",
    "    doc = ut.toks_to_spacy(nlp, sent)\n",
    "    res = []\n",
    "    for (i, token) in enumerate(doc):\n",
    "        next_w = doc[i+1] if i + 1 < len(doc) else None\n",
    "        prev_w = doc[i-1] if i - 1 >= 0 else None\n",
    "        res.append(to_features(token, prev_w, next_w))\n",
    "    return res\n",
    "def vectorize(dataset):\n",
    "    res = []\n",
    "    for sent in tqdm(dataset):\n",
    "        res += single_doc_to_features(sent)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans(sparse_matrix):\n",
    "    sparse_matrix.data = np.nan_to_num(sparse_matrix.data)\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293622/293622 [39:58<00:00, 123.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 195748/195748 [26:35<00:00, 122.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# dict_vectorizer = DictVectorizer()\n",
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.99      0.99   4894548\n",
      "       True       0.29      0.76      0.42     31865\n",
      "\n",
      "avg / total       0.99      0.99      0.99   4926413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.99      0.99   3242663\n",
      "       True       0.29      0.76      0.42     21299\n",
      "\n",
      "avg / total       0.99      0.99      0.99   3263962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it helps with gold test, even though it's kinda breaking the rules, but might be we overfitted on what we have in train/test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:02<00:00, 70.53it/s]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = predictor.predict(fill_nans(dict_vectorizer.transform(vectorize(test_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.98      0.99      4643\n",
      "       True       0.32      0.93      0.48        54\n",
      "\n",
      "avg / total       0.99      0.98      0.98      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_pred, flatten(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results seems way better. Now I'm gonna to change code in some of the functions used for creating / augmenting dataset in order to have more variation and actually less data. I assume it works that well because of spacy's `is_sent_start` attribute provided for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_positive_cases(Xs, ys, ratio=0.4, mergeThreeRatio=0.3, keep_ratio=0.05):\n",
    "    \"\"\"Merging randomly sampled sentences and adding positive cases\"\"\"\n",
    "    newXs = []\n",
    "    newYs = []\n",
    "    for i in range(math.floor(len(Xs) * ratio)):\n",
    "        first_id = random.randint(0, len(Xs) - 1)\n",
    "        second_id = random.randint(0, len(Xs) - 1)\n",
    "        while first_id == second_id:\n",
    "            second_id = random.randint(0, len(Xs) - 1)\n",
    "        first_X, first_y = Xs[first_id].copy(), ys[first_id].copy()\n",
    "        second_X, second_y = Xs[second_id].copy(), ys[second_id].copy()\n",
    "        first_X, first_y = remove_trailing_punctutation((first_X, first_y))\n",
    "        first_y[-1] = True\n",
    "        newXs.append(first_X + second_X)\n",
    "        newYs.append(first_y + second_y)\n",
    "    for i in range(math.floor(len(Xs) * mergeThreeRatio)):\n",
    "        first_id = random.randint(0, len(Xs) - 1)\n",
    "        second_id = random.randint(0, len(Xs) - 1)\n",
    "        while first_id == second_id:\n",
    "            second_id = random.randint(0, len(Xs) - 1)\n",
    "        third_id = random.randint(0, len(Xs) - 1)\n",
    "        while third_id == first_id or third_id == second_id:\n",
    "            third_id = random.randint(0, len(Xs) - 1)\n",
    "            \n",
    "        first_X, first_y = Xs[first_id].copy(), ys[first_id].copy()\n",
    "        second_X, second_y = Xs[second_id].copy(), ys[second_id].copy()\n",
    "        third_X, third_y = Xs[third_id].copy(), ys[third_id].copy()\n",
    "        first_X, first_y = remove_trailing_punctutation((first_X, first_y))\n",
    "        first_y[-1] = True\n",
    "        second_X, second_y = remove_trailing_punctutation((first_X, first_y))\n",
    "        second_y[-1] = True\n",
    "        newXs.append(first_X + second_X + third_X)\n",
    "        newYs.append(first_y + second_y + third_y)\n",
    "    for i in range(math.floor(len(Xs) * keep_ratio)):\n",
    "        index = random.randint(0, len(Xs) - 1)\n",
    "        newXs.append(Xs[index].copy())\n",
    "        newYs.append(ys[index].copy())\n",
    "    return newXs, newYs\n",
    "\n",
    "def replace_punct_tok(token):\n",
    "    if not is_finishing_punctutation(token):\n",
    "        return token\n",
    "    return random.choice(set(['.', '..', '...', '?', '!']) - set(token))\n",
    "\n",
    "def augment_partially(Xs, ys, ratio = 0.2):\n",
    "    \"\"\"Idea here was to do some augmentations on Xs like changing ending punctutation / names / farewells\"\"\"\n",
    "    newXs = []\n",
    "    newYs = []\n",
    "    subset_ratio = 0.4\n",
    "    subset_with_puncutation = []\n",
    "    k = 0\n",
    "    while len(subset_with_puncutation) < math.floor(len(Xs) * subset_ratio) and k < len(Xs):\n",
    "        if any([is_finishing_punctutation(tok) for tok in Xs[k]]):\n",
    "            subset_with_puncutation.append(k)\n",
    "        k += 1\n",
    "    for i in range(math.floor(len(Xs) * ratio)):\n",
    "        random_index = random.choice(subset_with_puncutation)\n",
    "        newX = [replace_punct_tok(x) for x in Xs[random_index].copy()]\n",
    "        newXs.append(newX)\n",
    "        newYs.append(ys[random_index].copy())\n",
    "    return Xs + newXs, ys + newYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413622"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs, ys = create_dataset(enrn_sentences_normalized + wiki_sentences_normalized + news_sentences, 0.4)\n",
    "len(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, my kernel has died on me, when I tried running vectorize on whole dataset after crunching through data for an hour, so I actually will skip experimenting with whole dataset and go to step 3 I planned for ‚Äì having smaller subset but trying more agressively to add positive cases / other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xs[0:50000], ys[0:50000], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35000/35000 [06:52<00:00, 84.78it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [02:28<00:00, 100.72it/s]\n"
     ]
    }
   ],
   "source": [
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))\n",
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99   1139204\n",
      "       True       0.43      0.71      0.53     21157\n",
      "\n",
      "avg / total       0.98      0.98      0.98   1160361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    492100\n",
      "       True       0.43      0.71      0.53      9051\n",
      "\n",
      "avg / total       0.98      0.98      0.98    501151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50000 samples on test will give:\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.99      0.98      0.99    492100\n",
    "       True       0.43      0.71      0.53      9051\n",
    "\n",
    "avg / total       0.98      0.98      0.98    501151\n",
    "```\n",
    "10000 samples on test will give:\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.99      0.98      0.99     98400\n",
    "       True       0.41      0.71      0.52      1749\n",
    "\n",
    "avg / total       0.98      0.98      0.98    100149\n",
    "```\n",
    "1000 samples on test will give:\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       1.00      0.98      0.99     10123\n",
    "       True       0.30      0.76      0.43       118\n",
    "\n",
    "avg / total       0.99      0.98      0.98     10241\n",
    "```\n",
    "100 samples on test will give:\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       1.00      0.98      0.99       955\n",
    "       True       0.30      0.69      0.42        13\n",
    "\n",
    "avg / total       0.99      0.97      0.98       968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet another model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few hypothesis:\n",
    "1. should probably split train/test before augmenting because otherwise lots of similar sentences might end up in both sets\n",
    "2. my input sentences might be not too meaningful for this task - should remove those consisting from one word and those that are over 25 tokens\n",
    "3. let's add to some subset of sentences words end of bye/ok thanks/etc\n",
    "4. let's try some additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_sentences, positive_cases_ratio, test_size=0.4, max_size=100):\n",
    "    size_train = math.floor(max_size*(1 - test_size))\n",
    "    size_test = math.floor(max_size*test_size)\n",
    "    raw_pairs = [ut.sentence_to_sequences(s) for s in dataset_sentences]\n",
    "    [Xs, ys] = zip(*raw_pairs)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=test_size, random_state=42)\n",
    "    \n",
    "    X_train = list(X_train)\n",
    "    y_train = list(y_train)\n",
    "    X_train, y_train = add_positive_cases(X_train, y_train, positive_cases_ratio)\n",
    "    X_train, y_train = augment_partially(X_train, y_train)\n",
    "    X_train, y_train = list(zip(*random.sample(list(zip(X_train, y_train)), size_train)))\n",
    "    \n",
    "    X_test = list(X_test)\n",
    "    y_test = list(y_test)\n",
    "    X_test, y_test = add_positive_cases(X_test, y_test, positive_cases_ratio)\n",
    "    X_test, y_test = augment_partially(X_test, y_test)\n",
    "    X_test, y_test = list(zip(*random.sample(list(zip(X_test, y_test)), size_test)))\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_dataset((enrn_sentences_normalized + wiki_sentences_normalized + news_sentences), 0.4, test_size=0.4, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 4000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222175"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [01:01<00:00, 98.12it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [00:40<00:00, 97.67it/s] \n"
     ]
    }
   ],
   "source": [
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((227227, 1014), 227227)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_X.shape, len(flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    222458\n",
      "       True       0.47      0.75      0.58      4769\n",
      "\n",
      "avg / total       0.98      0.98      0.98    227227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    148030\n",
      "       True       0.48      0.75      0.58      3241\n",
      "\n",
      "avg / total       0.98      0.98      0.98    151271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test result:\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      False       0.99      0.98      0.99    148030\n",
    "       True       0.48      0.75      0.58      3241\n",
    "\n",
    "avg / total       0.98      0.98      0.98    151271\n",
    "```\n",
    "which is higher than previously. I don't have a good explanation to why that happen, but several ideas:\n",
    "1) variance in data/predictor\n",
    "2) having smaller subset used both in test and train to create positive samples somehow causing this ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_sentence(sent):\n",
    "    return ' ' in sent and len(re.findall(' ', sent)) < 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = enrn_sentences_normalized + wiki_sentences_normalized + news_sentences\n",
    "filtered_sents = [sent for sent in all_sents if is_good_sentence(sent)]\n",
    "X_train, X_test, y_train, y_test = create_dataset(filtered_sents, 0.4, test_size=0.4, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:59<00:00, 101.50it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [00:39<00:00, 101.13it/s]\n"
     ]
    }
   ],
   "source": [
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))\n",
    "train_features_X.shape, len(flatten(y_train))\n",
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    198246\n",
      "       True       0.52      0.77      0.62      5768\n",
      "\n",
      "avg / total       0.98      0.97      0.98    204014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    132828\n",
      "       True       0.51      0.75      0.61      3922\n",
      "\n",
      "avg / total       0.98      0.97      0.97    136750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to slightly improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256987460278477"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_end():\n",
    "    return random.choice(['bye', 'farewell', 'thanks'])\n",
    "def random_start():\n",
    "    return random.choice(['hello', 'hi', 'hey', 'greetings'])\n",
    "def augment_partially(Xs, ys, ratio = 0.2):\n",
    "    \"\"\"Idea here was to do some augmentations on Xs like changing ending punctutation / names / farewells\"\"\"\n",
    "    newXs = []\n",
    "    newYs = []\n",
    "    subset_ratio = 0.4\n",
    "    subset_with_puncutation = []\n",
    "    k = 0\n",
    "    while len(subset_with_puncutation) < math.floor(len(Xs) * subset_ratio) and k < len(Xs):\n",
    "        if any([is_finishing_punctutation(tok) for tok in Xs[k]]):\n",
    "            subset_with_puncutation.append(k)\n",
    "        k += 1\n",
    "    for i in range(math.floor(len(Xs) * ratio)):\n",
    "        random_index = random.choice(subset_with_puncutation)\n",
    "        newX = [replace_punct_tok(x) for x in Xs[random_index].copy()]\n",
    "        newXs.append(newX)\n",
    "        newYs.append(ys[random_index].copy())\n",
    "    for i in range(math.floor(len(Xs) * ratio)):\n",
    "        random_index = random.randint(0, len(Xs) - 1)\n",
    "        if random.random() > 0.5:\n",
    "            newX = Xs[random_index].copy() + [random_end()]\n",
    "            newy = ys[random_index].copy() + [False]\n",
    "            newy[-2] = True\n",
    "        else:\n",
    "            newX = [random_start()] + Xs[random_index].copy()\n",
    "            newy = [True] + ys[random_index].copy()\n",
    "        newXs.append(newX)\n",
    "        newYs.append(newy)\n",
    "    return Xs + newXs, ys + newYs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using same params, give yet another few percents of improvement. I'm rerunning this. Given we know test is all about run-on sentences and we don't actually care for regular setences, let's have all of those in dataset be run-on for sure. Not something I would actually do if this was a task for production, but let's see, what this will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_dataset(filtered_sents, 1, test_size=0.4, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:58<00:00, 102.53it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [00:38<00:00, 103.49it/s]\n"
     ]
    }
   ],
   "source": [
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))\n",
    "train_features_X.shape, len(flatten(y_train))\n",
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    189300\n",
      "       True       0.47      0.76      0.58      4806\n",
      "\n",
      "avg / total       0.98      0.97      0.98    194106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    126467\n",
      "       True       0.45      0.75      0.56      3099\n",
      "\n",
      "avg / total       0.98      0.97      0.98    129566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually made results worse. rolling back to 0.4 ratio for next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_dataset(filtered_sents, 0.4, test_size=0.4, max_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For features, let's check\n",
    "1. adding another word prior and afterwards\n",
    "2. trying to infer distance for supposedly start/end of sentence for current word\n",
    "3. check whether head of token is left or right of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_tok_to_feature(label, tok):\n",
    "    if not tok:\n",
    "        return {\n",
    "            label + '_shape': 'NONE',\n",
    "            label + '_is_quote': False,\n",
    "            label + '_is_punct': False,\n",
    "            label + '_is_start': False,\n",
    "            label + '_in_vocab': False,\n",
    "            label + '_like_num': False,\n",
    "            label + '_like_url': False,\n",
    "            label + '_like_email': False,\n",
    "            label + '_ent_type': 'NONE',\n",
    "        }\n",
    "    return {\n",
    "        label + '_shape': tok.shape_[0:4],\n",
    "        label + '_is_quote': is_quote(str(tok)),\n",
    "        label + '_is_punct': tok.is_punct,\n",
    "        label + '_is_start': tok.is_sent_start,\n",
    "        label + '_in_vocab': str(tok) in nlp.vocab,\n",
    "        label + '_like_num': tok.like_num,\n",
    "        label + '_like_url': tok.like_url,\n",
    "        label + '_like_email': tok.like_email,\n",
    "        label + '_ent_type': tok.ent_type_\n",
    "        \n",
    "    }\n",
    "def to_features(word, prev_1, next_1, prev_2, next_2, after_sent_start, till_send_end): # prev_2, prev_3, next_2, next_3\n",
    "    return {\n",
    "        'after_sent_start': after_sent_start,\n",
    "        'till_send_end': till_send_end,\n",
    "        'head_right': word.i < word.head.i,\n",
    "        'head_left': word.i > word.head.i,\n",
    "        **single_tok_to_feature('word', word),\n",
    "        **single_tok_to_feature('prev_1', prev_1),\n",
    "        **single_tok_to_feature('prev_2', prev_2),\n",
    "        **single_tok_to_feature('next_1', next_1),\n",
    "        **single_tok_to_feature('next_2', next_2)\n",
    "    }\n",
    "def single_doc_to_features(sentence):\n",
    "    sent = [tok for tok in sentence if tok]\n",
    "    doc = ut.toks_to_spacy(nlp, sent)\n",
    "    res = []\n",
    "    after_sent_start = 0\n",
    "    till_send_end = len(doc)\n",
    "    sent_ends = [tok.is_sent_start for tok in doc]\n",
    "    for (i, token) in enumerate(doc):\n",
    "        after_sent_start += 1\n",
    "        if token.is_sent_start:\n",
    "            after_sent_start = 0\n",
    "        try:\n",
    "            till_send_end = sent_ends[i:].index(True)\n",
    "        except ValueError:\n",
    "            till_send_end = len(sent_ends[i:])\n",
    "        next_w = doc[i+1] if i + 1 < len(doc) else None\n",
    "        next_w2 = doc[i+2] if i + 2 < len(doc) else None\n",
    "        prev_w = doc[i-1] if i - 1 >= 0 else None\n",
    "        prev_w2 = doc[i-2] if i - 2 >= 0 else None\n",
    "        res.append(to_features(token, prev_1=prev_w, prev_2=prev_w2, next_1=next_w, next_2=next_w2, after_sent_start=after_sent_start, till_send_end=till_send_end))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [01:06<00:00, 94.89it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [00:43<00:00, 92.94it/s] \n"
     ]
    }
   ],
   "source": [
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))\n",
    "train_features_X.shape, len(flatten(y_train))\n",
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99    196929\n",
      "       True       0.68      0.85      0.76      6921\n",
      "\n",
      "avg / total       0.98      0.98      0.98    203850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99    133053\n",
      "       True       0.67      0.84      0.75      4568\n",
      "\n",
      "avg / total       0.98      0.98      0.98    137621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems improve to results. I think I'm done here, so just for fun, gonna take slightly more data in, train classifier and predict on gold test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [10:51<00:00, 92.04it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40000/40000 [07:34<00:00, 87.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99   1982251\n",
      "       True       0.69      0.84      0.76     70487\n",
      "\n",
      "avg / total       0.98      0.98      0.98   2052738\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99   1327044\n",
      "       True       0.69      0.84      0.76     47494\n",
      "\n",
      "avg / total       0.98      0.98      0.98   1374538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = create_dataset(filtered_sents, 0.4, test_size=0.4, max_size=100000)\n",
    "vec_train_X = vectorize(X_train)\n",
    "vec_test_X = vectorize(X_test)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "train_features_X = fill_nans(dict_vectorizer.fit_transform(vec_train_X))\n",
    "test_features_X = fill_nans(dict_vectorizer.transform(vec_test_X))\n",
    "predictor = LogisticRegression().fit(X=train_features_X, y=flatten(y_train))\n",
    "print(classification_report(predictor.predict(train_features_X), flatten(y_train)))\n",
    "print(classification_report(predictor.predict(test_features_X), flatten(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:03<00:00, 50.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Okay, let's try evaluating on test\n",
    "y_test_pred = predictor.predict(fill_nans(dict_vectorizer.transform(vectorize(test_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.98      0.99      4616\n",
      "       True       0.42      0.80      0.55        81\n",
      "\n",
      "avg / total       0.99      0.98      0.98      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_pred, flatten(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems to be 0.39 improvement in precision compared to baseline and 0.74 in recall  \n",
    "in terms of second base model trained on sentences from enron we only get 0.1 improvement in precision and 0.13 dive in recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
